{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7683fc71-acc4-47e0-af53-4a0d285439fc",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "# Fine-tuning a BERT model on a negation scope detection task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbf978c-9abf-4b46-8535-4b27fc4c5f07",
   "metadata": {
    "id": "X4cRE8IbIrIV"
   },
   "source": [
    "Let's install the packages we need from ðŸ¤— : Transformers, Datasets, Seqeval, and Evalaute, and all the necessary packages for preprocessing and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccd0296-121e-4575-92c6-e840c113fc8c",
   "metadata": {},
   "source": [
    "**Please find the saved model, tokenizer and trainer at my Google Drive: https://drive.google.com/drive/folders/16HOQszOappcrxuOCChbAesLI10aHlcuy?usp=drive_link**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fff118b-162b-4153-ba05-855e57497bb1",
   "metadata": {
    "id": "IreSlFmlIrIm"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import set_seed\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "47d30cce-2391-45a7-b94c-5a12f2a25c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative paths to all files, only DATA_DIR needs to be changed\n",
    "DATA_DIR = '../../starsem-st-2012-data/starsem-st-2012-data/cd-sco/corpus/'\n",
    "train = DATA_DIR + 'training/SEM-2012-SharedTask-CD-SCO-training-09032012.txt'\n",
    "dev = DATA_DIR+'dev/SEM-2012-SharedTask-CD-SCO-dev-09032012.txt'\n",
    "test_card = DATA_DIR+'test-gold/SEM-2012-SharedTask-CD-SCO-test-cardboard-GOLD.txt'\n",
    "test_circle = DATA_DIR+'test-gold/SEM-2012-SharedTask-CD-SCO-test-circle-GOLD.txt'\n",
    "ann17 = DATA_DIR+'file-17-to-annotate.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5854f5b-c668-4814-bc8f-acfd6689acf2",
   "metadata": {},
   "source": [
    "## Duplication script for any number of negation cues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "db702420-a43e-4601-9732-6c8136a0ca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dupfile(inputfile, outputfile, rmgold=False):\n",
    "    \"\"\"\n",
    "    Processes a dataset with variable columns and duplicates sentences based on the number of negation cues.\n",
    "\n",
    "    Parameters:\n",
    "    inputfile: path to a text file from the dataset\n",
    "    outputfile: path to a new file with 10 columns and duplicated sentences\n",
    "    rmgold: Boolean value set to False as default. if set to True, gold labels and syntax are removed from the new file\n",
    "\n",
    "    This function processes a file with any number of negation cues and creates duplicates in the sentences where\n",
    "    more than one negation is found. \n",
    "    If \"cue1\" starts with \"***\", the sentence is removed from our dataset as our task involves searching for a negation scope only when a cue is given.\n",
    "\n",
    "    If the parameter 'rmgold' is set to True, it removes the gold labels and syntax similarly to the file given in appendix B\n",
    "    of assignment 1. The default value is False, and if skipped, the function only duplicates the necessary sentences\n",
    "    without removing any labels.\n",
    "\n",
    "    The original and duplicated sentences are written to the new file and the function returns nothing.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    # defining fixed column names\n",
    "    fixed_columns = [\n",
    "        'chapter_name', 'sentence_number', 'token_number', 'word', 'lemma',\n",
    "        'part-of-speech', 'syntax'\n",
    "    ]\n",
    "\n",
    "    # updating the maximum possible number of columns by checking the length of each row in the dataset\n",
    "    rows = []\n",
    "    max_columns = 0\n",
    "\n",
    "    with open(inputfile, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if line.strip():  # skipping empty lines\n",
    "                row = line.strip().split('\\t')\n",
    "                rows.append(row)\n",
    "                max_columns = max(max_columns, len(row))\n",
    "\n",
    "    # generating dynamic column names\n",
    "    num_fixed_columns = len(fixed_columns)\n",
    "    num_dynamic_columns = max_columns - num_fixed_columns\n",
    "    # if the columns are not 0 and if they can be divided by 3\n",
    "    if num_dynamic_columns % 3 != 0:\n",
    "        raise ValueError(\"Negation columns are not in multiples of 3.\")\n",
    "    num_cues = num_dynamic_columns // 3\n",
    "    dynamic_columns = [\n",
    "        f'{name}{i + 1}' for i in range(num_cues) for name in ['cue', 'scope', 'event']\n",
    "    ]\n",
    "\n",
    "    # combining fixed and dynamic columns\n",
    "    column_names = fixed_columns + dynamic_columns\n",
    "\n",
    "    # creating a df with the parsed rows and dynamic columns\n",
    "    df = pd.DataFrame(rows, columns=column_names).fillna('')\n",
    "\n",
    "    # grouping sentences by chapter name and sentence number and disabling automatic sorting\n",
    "    sentences = df.groupby(['chapter_name', 'sentence_number'], sort=False)\n",
    "\n",
    "    # defining output columns\n",
    "    output_columns = [\n",
    "        'chapter_name', 'sentence_number', 'token_number', 'word', 'lemma',\n",
    "        'part-of-speech', 'syntax', 'cue', 'scope', 'event'\n",
    "    ]\n",
    "\n",
    "    # writing sentences to the output file\n",
    "    with open(outputfile, 'w', encoding='utf-8') as newfile:\n",
    "        for (chapter_name, sentence_number), group in sentences:\n",
    "\n",
    "            if (group['cue1'] == '***').any():\n",
    "                continue\n",
    "\n",
    "            # removing gold labels and syntax if rmgold is True\n",
    "            if rmgold:\n",
    "                group['syntax'] = '_'\n",
    "                for col in dynamic_columns:\n",
    "                    if col.startswith('scope') or col.startswith('event'):\n",
    "                        group[col] = '_'\n",
    "\n",
    "            duplications = []\n",
    "\n",
    "            # processing each cue dynamically\n",
    "            for i in range(num_cues):\n",
    "                cue_col = f'cue{i + 1}'\n",
    "                scope_col = f'scope{i + 1}'\n",
    "                event_col = f'event{i + 1}'\n",
    "\n",
    "                if cue_col in group and not group[cue_col].empty:\n",
    "                    # checking if the cue column contains alphabetic content or negation or '***'\n",
    "                    if group[cue_col].fillna('').astype(str).apply(str.isalpha).any() or (group[cue_col] == \"n't\").any() or (group[cue_col] == \"***\").any():\n",
    "                        duplication = group.copy()\n",
    "                        duplication['sentence_number'] = f\"{\n",
    "                            sentence_number}_{i + 1}\"\n",
    "                        duplication['cue'] = duplication[cue_col]\n",
    "                        duplication['scope'] = duplication[scope_col]\n",
    "                        duplication['event'] = duplication[event_col]\n",
    "                        duplication = duplication[output_columns]\n",
    "                        duplications.append(duplication)\n",
    "\n",
    "            # writing each duplication\n",
    "            for dup in duplications:\n",
    "                dup.to_csv(newfile, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6e40e128-5c9b-432b-a6b3-95c0f0318e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to True if you don't want gold labels and syntax removed\n",
    "create_dupfile(dev, 'duplicated_dev.txt', rmgold=False)\n",
    "create_dupfile(test_card, 'duplicated_test_card.txt', rmgold=False)\n",
    "create_dupfile(test_circle, 'duplicated_test_circle.txt', rmgold=False)\n",
    "create_dupfile(ann17, 'duplicated_anno.csv', rmgold=False)\n",
    "create_dupfile(train, 'duplicated_train.txt', rmgold=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5700a373-8b31-47fd-a354-4acf5be02143",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_train = 'duplicated_train.txt'\n",
    "dup_dev = 'duplicated_dev.txt'\n",
    "dup_test_card = 'duplicated_test_card.txt'\n",
    "dup_test_circle = 'duplicated_test_circle.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cb95864e-c693-4621-837d-1f4830434732",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dup_combined_test = 'cleaned_dup_combined_test.txt'\n",
    "\n",
    "# combining the two test files\n",
    "with open(cleaned_dup_combined_test, 'w') as new_file:\n",
    "    with open(dup_test_circle, 'r') as file1:\n",
    "        for line in file1:\n",
    "            if line.strip():\n",
    "                new_file.write(line)\n",
    "\n",
    "    with open(dup_test_card, 'r') as file2:\n",
    "        for line in file2:\n",
    "            if line.strip():\n",
    "                new_file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb2a14f-6547-4818-9302-7a9b3dcfe3df",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8c9bef79-7781-4a9f-874e-d12f32d8e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll(file):\n",
    "    \"\"\"\n",
    "    Reads a CoNLL-formatted file and extracts linguistic annotations into a structured list of dictionaries.\n",
    "\n",
    "    Each line in the file represents a token with multiple attributes, separated by tab spaces. The function \n",
    "    parses these attributes dynamically to handle any number of negation cues, scopes, and events, organizing \n",
    "    them into dictionaries, one per token.\n",
    "\n",
    "    Parameters:\n",
    "        file (str): Path to the CoNLL-formatted file to be read.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries where each dictionary corresponds to a line in the file and \n",
    "        contains the following keys:\n",
    "            - 'chapter' (str): Chapter identifier for the token.\n",
    "            - 'sent_num' (str): Sentence number within the chapter.\n",
    "            - 'word_num' (str): Token number within the sentence.\n",
    "            - 'word' (str): The actual word/token.\n",
    "            - 'lemma' (str): Lemmatized form of the token.\n",
    "            - 'pos' (str): Part-of-speech tag of the token.\n",
    "            - 'syntax' (str): Syntactic role of the token.\n",
    "            - 'negX' (str): Negation cue for each dynamic index X.\n",
    "            - 'scopeX' (str or None): Scope for the corresponding negation cue X, if available.\n",
    "            - 'eventX' (str or None): Event for the corresponding negation cue X, if available.\n",
    "\n",
    "    Notes:\n",
    "        - The function dynamically handles any number of negation cue, scope, and event columns.\n",
    "        - If a line has fewer than the expected columns, missing values are set to 'None'.\n",
    "\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(file, 'r', encoding='utf8') as infile:\n",
    "        for line in infile:\n",
    "            components = line.rstrip('\\n').split()\n",
    "            if len(components) > 0:\n",
    "                # Core attributes\n",
    "                chapter, sent_num, word_num, word, lemma, pos, syntax = components[:7]\n",
    "                dynamic_fields = components[7:]\n",
    "\n",
    "                # dynamically extract neg, scope, and event columns\n",
    "                # ensure 'none' is treated as a valid value (as one of the negation cue)\n",
    "                if dynamic_fields:\n",
    "                    neg_cues = len(dynamic_fields) // 3\n",
    "                    cols = {\n",
    "                        'chapter': chapter,\n",
    "                        'sent_num': sent_num,\n",
    "                        'word_num': word_num,\n",
    "                        'word': word,\n",
    "                        'lemma': lemma,\n",
    "                        'pos': pos,\n",
    "                        'syntax': syntax\n",
    "                    }\n",
    "                    for i in range(neg_cues):\n",
    "                        neg_idx = i * 3\n",
    "                        neg_value = dynamic_fields[neg_idx].strip().lower()\n",
    "                        cols[f'neg{i + 1}'] = neg_value if neg_value != 'None' else None  # exclude uppercase 'None'\n",
    "                        cols[f'scope{i + 1}'] = dynamic_fields[neg_idx + 1]\n",
    "                        cols[f'event{i + 1}'] = dynamic_fields[neg_idx + 2]\n",
    "                    data.append(cols)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "22af8e8e-1467-4a92-a332-b7ed7b1f4c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sents(df):\n",
    "    \"\"\"\n",
    "    Calculates the total number of unique sentences across all chapters in a given DataFrame.\n",
    "\n",
    "    The function assumes the DataFrame contains columns named 'chapter' and 'sent_num', where:\n",
    "    - 'chapter' identifies the chapter each row belongs to.\n",
    "    - 'sent_num' identifies the sentence number within the chapter.\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): A Pandas DataFrame of the data\n",
    "\n",
    "    Returns:\n",
    "        int: The total number of unique sentences across all chapters in the DataFrame.\n",
    "\n",
    "    Notes:\n",
    "        - The function uses `set` to determine unique sentence numbers within each chapter.\n",
    "        - The column names in the DataFrame must match 'chapter' and 'sent_num' exactly.\n",
    "\n",
    "    \"\"\"\n",
    "    chapters = list(set(list(df['chapter'])))\n",
    "\n",
    "    count = 0\n",
    "    for chapter in chapters:\n",
    "        # chcks the unique sent numbers for every chapter and adds it to the count of all sentences\n",
    "        count += len(set(df[df['chapter'] == chapter]['sent_num']))\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8d0d1d9d-292f-4e42-880d-9134be590d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_stats(df):\n",
    "    \"\"\"\n",
    "    Computes various statistics related to negation in a given DataFrame.\n",
    "\n",
    "    This function analyzes negation cues and their scopes in the dataset, dynamically processing \n",
    "    any number of negation-related columns. It produces a dictionary with information about the \n",
    "    number of sentences, words, negation cues, and their distributions.\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): A DataFrame containing at least the following columns:\n",
    "            - 'chapter' (str): Chapter identifier.\n",
    "            - 'sent_num' (str): Sentence number within the chapter.\n",
    "            - 'word' (str): Token in the sentence.\n",
    "            - 'negX' (str): Negation cues for each dynamic index X, where '_', '***', and None indicate absence.\n",
    "            - 'scopeX' (str): Scope annotations for each dynamic index X, where '_', '***', and None indicate absence.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the following statistics:\n",
    "            - 'num_sent' (int): Total number of sentences in the DataFrame.\n",
    "            - 'num_words' (int): Total number of words in the DataFrame.\n",
    "            - 'num_neg_cue_ins' (int): Number of negation cue instances.\n",
    "            - 'num_uni_neg_cue' (int): Number of unique negation cue forms.\n",
    "            - 'num_neg_cue_tokens' (int): Total number of tokens used in all negation cues.\n",
    "            - 'num_token_in_scope' (int): Number of tokens inside a negation scope.\n",
    "            - 'num_token_out_scope' (int): Number of tokens outside a negation scope.\n",
    "            - 'neg_distribution' (Counter): Distribution of negation cues (frequency of each negation cue).\n",
    "\n",
    "    Notes:\n",
    "        - The function dynamically identifies negation-related columns (negX, scopeX, etc.).\n",
    "        - Negation cue keys are tracked as 'chapter_sent_num_negX' to track their occurrences.\n",
    "\n",
    "    \"\"\"\n",
    "    neg_columns = [col for col in df.columns if col.startswith(\"neg\")]\n",
    "    scope_columns = [col for col in df.columns if col.startswith(\"scope\")]\n",
    "\n",
    "    # ensure all neg_columns are strings\n",
    "    for neg_col in neg_columns:\n",
    "        # replace 'nan', '_', '***') with empty strings ''\n",
    "        df[neg_col] = df[neg_col].astype(str).replace({'nan': '', '_': '', '***': ''})\n",
    "\n",
    "    # negation cues statistics\n",
    "    final_dict = {}\n",
    "    for neg_col in neg_columns:\n",
    "        for index, row in df.iterrows():\n",
    "            neg_value = row[neg_col].strip()\n",
    "            if neg_value and neg_value not in ('_', '', '***', 'None'):  # keep valid 'none' to avoid missed count in Test Circle\n",
    "                neg_value = neg_value.lower()\n",
    "                key = f\"{row['chapter']}_{row['sent_num']}_{neg_col}\"\n",
    "                if key in final_dict:\n",
    "                    final_dict[key] += f\" {neg_value}\"\n",
    "                else:\n",
    "                    final_dict[key] = neg_value\n",
    "\n",
    "\n",
    "    # total number of negation cue tokens\n",
    "    count = sum(len(cue.split()) for cue in final_dict.values())\n",
    "\n",
    "    # Scope statistics\n",
    "    in_count = 0\n",
    "    out_count = 0\n",
    "    for index, row in df.iterrows():\n",
    "        inside_scope = False #check if the token is inside any scope\n",
    "        for scope_col in scope_columns:\n",
    "            #pd.notna() returns True if the value is valid (not NaN or None)\n",
    "            scope_value = str(row[scope_col]).strip() if pd.notna(row[scope_col]) else ''\n",
    "            if scope_value not in ('_', '', '***', 'none'):\n",
    "                inside_scope = True\n",
    "                break # if the scope value is valid, stop checking further\n",
    "        if inside_scope:\n",
    "            in_count += 1\n",
    "        else:\n",
    "            out_count += 1\n",
    "\n",
    "    # Compile statistics\n",
    "    data = {\n",
    "        'num_sent': find_sents(df),\n",
    "        'num_words': len(df),\n",
    "        'num_neg_cue_ins': len(final_dict),\n",
    "        'num_uni_neg_cue': len(set(final_dict.values())),\n",
    "        'num_neg_cue_tokens': count,\n",
    "        'num_token_in_scope': in_count,\n",
    "        'num_token_out_scope': out_count,\n",
    "        'neg_distribution': Counter(final_dict.values()),\n",
    "    }\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\"\"\n",
    "    {\"Total number of sentences in the Text:\":<52} {data['num_sent']}\n",
    "    {\"Total number of words in the Text:\":<52} {data['num_words']}\n",
    "    {\"Number of negation cue instances:\":<52} {data['num_neg_cue_ins']}\n",
    "    {\"Number of unique negation cue forms:\":<52} {data['num_uni_neg_cue']}\n",
    "    {\"Total number of tokens used in all negation cues:\":<52} {data['num_neg_cue_tokens']}\n",
    "    {\"Number of tokens inside a negation scope:\":<52} {data['num_token_in_scope']}\n",
    "    {\"Number of tokens outside a negation scope:\":<52} {data['num_token_out_scope']}\n",
    "    Distribution of negation cues (frequency of each negation cue):\n",
    "    \"\"\")\n",
    "\n",
    "    max_cue_length = max(len(cue) for cue in data['neg_distribution'].keys()) if data['neg_distribution'] else 0\n",
    "    for cue, count in data['neg_distribution'].items():\n",
    "        print(f\"{cue:<{max_cue_length}} : {count}\")\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "899e4fc4-d62c-4e95-8718-af95b11cf89c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialising dataframes for all files, the csv file is given column names to reference data\n",
    "\n",
    "# Before Duplication\n",
    "dev_df = pd.DataFrame.from_dict(read_conll(dev))\n",
    "test_card_df = pd.DataFrame.from_dict(read_conll(test_card))\n",
    "test_circle_df = pd.DataFrame.from_dict(read_conll(test_circle))\n",
    "train_df = pd.DataFrame.from_dict(read_conll(train))\n",
    "\n",
    "ann17_df = pd.read_csv(ann17, delimiter='\\t', header=None, names=[\n",
    "                       'chapter', 'sent_num', 'word_num', 'word', 'lemma', 'pos', 'syntax',\n",
    "                       'neg1', 'scope1', 'event1', 'neg2', 'scope2', 'event2', 'neg3', 'scope3', 'event3'], keep_default_na=False, na_values=[\"\"])\n",
    "ann17_df = ann17_df.map(lambda x: None if pd.isna(x) else x)\n",
    "\n",
    "# keep_default_na = False -- we do this as some words like \"None\" were read as not available\n",
    "# na_values=[\"\"] -- this makes sure we still read \"\" as not available data and all \"\" become NaN\n",
    "# ann17_df = ann17_df.map(lambda x: None if pd.isna(x) else x) -- this converts all NaN into None type data which we have conditions set up for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2d5d4a1b-848c-4903-890c-5760cb231ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev:\n",
      "\n",
      "    Total number of sentences in the Text:               787\n",
      "    Total number of words in the Text:                   13567\n",
      "    Number of negation cue instances:                    173\n",
      "    Number of unique negation cue forms:                 20\n",
      "    Total number of tokens used in all negation cues:    179\n",
      "    Number of tokens inside a negation scope:            1290\n",
      "    Number of tokens outside a negation scope:           12277\n",
      "    Distribution of negation cues (frequency of each negation cue):\n",
      "    \n",
      "no          : 32\n",
      "not         : 42\n",
      "un          : 16\n",
      "never       : 11\n",
      "im          : 6\n",
      "n't         : 20\n",
      "neither nor : 2\n",
      "nothing     : 16\n",
      "without     : 7\n",
      "no nor      : 1\n",
      "by no means : 1\n",
      "neither     : 1\n",
      "ir          : 1\n",
      "in          : 5\n",
      "nor         : 4\n",
      "save        : 1\n",
      "less        : 3\n",
      "dis         : 2\n",
      "nobody      : 1\n",
      "no more     : 1\n",
      "\n",
      "\n",
      "\n",
      "Test Card:\n",
      "\n",
      "    Total number of sentences in the Text:               496\n",
      "    Total number of words in the Text:                   10184\n",
      "    Number of negation cue instances:                    133\n",
      "    Number of unique negation cue forms:                 15\n",
      "    Total number of tokens used in all negation cues:    135\n",
      "    Number of tokens inside a negation scope:            902\n",
      "    Number of tokens outside a negation scope:           9282\n",
      "    Distribution of negation cues (frequency of each negation cue):\n",
      "    \n",
      "im          : 3\n",
      "no          : 25\n",
      "un          : 11\n",
      "neither nor : 1\n",
      "in          : 4\n",
      "not         : 41\n",
      "far from    : 1\n",
      "without     : 5\n",
      "less        : 2\n",
      "ir          : 1\n",
      "nothing     : 8\n",
      "n't         : 18\n",
      "dis         : 1\n",
      "never       : 11\n",
      "nor         : 1\n",
      "\n",
      "\n",
      "\n",
      "Test Circle:\n",
      "\n",
      "    Total number of sentences in the Text:               593\n",
      "    Total number of words in the Text:                   9032\n",
      "    Number of negation cue instances:                    131\n",
      "    Number of unique negation cue forms:                 17\n",
      "    Total number of tokens used in all negation cues:    139\n",
      "    Number of tokens inside a negation scope:            829\n",
      "    Number of tokens outside a negation scope:           8203\n",
      "    Distribution of negation cues (frequency of each negation cue):\n",
      "    \n",
      "not                : 41\n",
      "never              : 9\n",
      "n't                : 16\n",
      "no                 : 25\n",
      "nothing            : 14\n",
      "neither nor nor    : 1\n",
      "un                 : 8\n",
      "none               : 1\n",
      "no more            : 2\n",
      "without            : 2\n",
      "neither nor        : 2\n",
      "nor                : 2\n",
      "less               : 3\n",
      "dis                : 2\n",
      "absolutely nothing : 1\n",
      "ir                 : 1\n",
      "never more         : 1\n",
      "\n",
      "\n",
      "\n",
      "17 Sentences to annotate:\n",
      "\n",
      "    Total number of sentences in the Text:               17\n",
      "    Total number of words in the Text:                   410\n",
      "    Number of negation cue instances:                    32\n",
      "    Number of unique negation cue forms:                 11\n",
      "    Total number of tokens used in all negation cues:    35\n",
      "    Number of tokens inside a negation scope:            0\n",
      "    Number of tokens outside a negation scope:           410\n",
      "    Distribution of negation cues (frequency of each negation cue):\n",
      "    \n",
      "not         : 11\n",
      "un          : 3\n",
      "n't         : 6\n",
      "no          : 1\n",
      "neither nor : 1\n",
      "by no means : 1\n",
      "nor         : 3\n",
      "nothing     : 1\n",
      "never       : 2\n",
      "im          : 1\n",
      "without     : 2\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [{'val': dev_df, 'name': 'Dev'}, {'val': test_card_df, 'name': 'Test Card'}, {'val': test_circle_df, 'name': 'Test Circle'}, {'val': ann17_df, 'name': '17 Sentences to annotate'}]:\n",
    "    print(i['name']+':')\n",
    "    find_stats(i['val'])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "825a975e-078f-4c23-a348-15503c05c1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After Duplication -- please run only after files have been generated in the same directory as notebook\n",
    "dup_dev_df = pd.DataFrame.from_dict(read_conll('duplicated_dev.txt'))\n",
    "dup_test_card_df = pd.DataFrame.from_dict(\n",
    "    read_conll('duplicated_test_card.txt'))\n",
    "dup_test_circle_df = pd.DataFrame.from_dict(\n",
    "    read_conll('duplicated_test_circle.txt'))\n",
    "dup_train_df = pd.DataFrame.from_dict(read_conll('duplicated_train.txt'))\n",
    "dup_all_test_df = pd.DataFrame.from_dict(read_conll(cleaned_dup_combined_test))\n",
    "dup_ann17_df = pd.read_csv('duplicated_anno.csv', delimiter='\\t', header=None, names=[\n",
    "    'chapter', 'sent_num', 'word_num', 'word', 'lemma', 'pos', 'syntax',\n",
    "    'neg1', 'scope1', 'event1', 'neg2', 'scope2', 'event2', 'neg3', 'scope3', 'event3'], keep_default_na=False, na_values=[\"\"])\n",
    "dup_ann17_df = dup_ann17_df.map(lambda x: None if pd.isna(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cfdf54e7-e8b8-4d2c-a567-32b6d00d4ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev:\n",
      "\n",
      "    Total number of sentences in the Text:               173\n",
      "    Total number of words in the Text:                   3592\n",
      "    Number of negation cue instances:                    173\n",
      "    Number of unique negation cue forms:                 20\n",
      "    Total number of tokens used in all negation cues:    179\n",
      "    Number of tokens inside a negation scope:            1368\n",
      "    Number of tokens outside a negation scope:           2224\n",
      "    Distribution of negation cues (frequency of each negation cue):\n",
      "    \n",
      "no          : 32\n",
      "not         : 42\n",
      "un          : 16\n",
      "never       : 11\n",
      "im          : 6\n",
      "without     : 7\n",
      "n't         : 20\n",
      "neither nor : 2\n",
      "nothing     : 16\n",
      "nor         : 4\n",
      "no nor      : 1\n",
      "by no means : 1\n",
      "neither     : 1\n",
      "ir          : 1\n",
      "in          : 5\n",
      "less        : 3\n",
      "save        : 1\n",
      "dis         : 2\n",
      "nobody      : 1\n",
      "no more     : 1\n",
      "\n",
      "\n",
      "\n",
      "Test Card:\n",
      "\n",
      "    Total number of sentences in the Text:               133\n",
      "    Total number of words in the Text:                   3130\n",
      "    Number of negation cue instances:                    133\n",
      "    Number of unique negation cue forms:                 15\n",
      "    Total number of tokens used in all negation cues:    135\n",
      "    Number of tokens inside a negation scope:            963\n",
      "    Number of tokens outside a negation scope:           2167\n",
      "    Distribution of negation cues (frequency of each negation cue):\n",
      "    \n",
      "im          : 3\n",
      "not         : 41\n",
      "no          : 25\n",
      "un          : 11\n",
      "neither nor : 1\n",
      "in          : 4\n",
      "far from    : 1\n",
      "without     : 5\n",
      "less        : 2\n",
      "nothing     : 8\n",
      "ir          : 1\n",
      "n't         : 18\n",
      "dis         : 1\n",
      "never       : 11\n",
      "nor         : 1\n",
      "\n",
      "\n",
      "\n",
      "Test Circle:\n",
      "\n",
      "    Total number of sentences in the Text:               131\n",
      "    Total number of words in the Text:                   2580\n",
      "    Number of negation cue instances:                    131\n",
      "    Number of unique negation cue forms:                 17\n",
      "    Total number of tokens used in all negation cues:    139\n",
      "    Number of tokens inside a negation scope:            863\n",
      "    Number of tokens outside a negation scope:           1717\n",
      "    Distribution of negation cues (frequency of each negation cue):\n",
      "    \n",
      "not                : 41\n",
      "un                 : 8\n",
      "nor                : 2\n",
      "never              : 9\n",
      "n't                : 16\n",
      "no                 : 25\n",
      "no more            : 2\n",
      "nothing            : 14\n",
      "neither nor nor    : 1\n",
      "none               : 1\n",
      "absolutely nothing : 1\n",
      "without            : 2\n",
      "ir                 : 1\n",
      "neither nor        : 2\n",
      "less               : 3\n",
      "dis                : 2\n",
      "never more         : 1\n",
      "\n",
      "\n",
      "\n",
      "17 Sentences to annotate:\n",
      "\n",
      "    Total number of sentences in the Text:               32\n",
      "    Total number of words in the Text:                   793\n",
      "    Number of negation cue instances:                    32\n",
      "    Number of unique negation cue forms:                 11\n",
      "    Total number of tokens used in all negation cues:    35\n",
      "    Number of tokens inside a negation scope:            0\n",
      "    Number of tokens outside a negation scope:           793\n",
      "    Distribution of negation cues (frequency of each negation cue):\n",
      "    \n",
      "not         : 11\n",
      "un          : 3\n",
      "n't         : 6\n",
      "no          : 1\n",
      "never       : 2\n",
      "neither nor : 1\n",
      "nor         : 3\n",
      "by no means : 1\n",
      "im          : 1\n",
      "without     : 2\n",
      "nothing     : 1\n",
      "\n",
      "\n",
      "\n",
      "Combined test data:\n",
      "\n",
      "    Total number of sentences in the Text:               264\n",
      "    Total number of words in the Text:                   5710\n",
      "    Number of negation cue instances:                    264\n",
      "    Number of unique negation cue forms:                 20\n",
      "    Total number of tokens used in all negation cues:    274\n",
      "    Number of tokens inside a negation scope:            1826\n",
      "    Number of tokens outside a negation scope:           3884\n",
      "    Distribution of negation cues (frequency of each negation cue):\n",
      "    \n",
      "not                : 82\n",
      "un                 : 19\n",
      "nor                : 3\n",
      "never              : 20\n",
      "n't                : 34\n",
      "no                 : 50\n",
      "no more            : 2\n",
      "nothing            : 22\n",
      "neither nor nor    : 1\n",
      "none               : 1\n",
      "absolutely nothing : 1\n",
      "without            : 7\n",
      "ir                 : 2\n",
      "neither nor        : 3\n",
      "less               : 5\n",
      "dis                : 3\n",
      "never more         : 1\n",
      "im                 : 3\n",
      "in                 : 4\n",
      "far from           : 1\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [{'val': dup_dev_df, 'name': 'Dev'}, {'val': dup_test_card_df, 'name': 'Test Card'}, {'val': dup_test_circle_df, 'name': 'Test Circle'},\n",
    "          {'val': dup_ann17_df, 'name': '17 Sentences to annotate'}, {'val': dup_all_test_df, 'name': 'Combined test data'}]:\n",
    "    print(i['name']+':')\n",
    "    find_stats(i['val'])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91da6bfe-acf8-428b-9787-72f19f322107",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0117deb0-8b54-40f2-8f09-4b5a59eabe5b",
   "metadata": {
    "id": "zVvslsfMIrIh"
   },
   "outputs": [],
   "source": [
    "task = \"scope_detection\"\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2d558f0e-590a-495c-abf0-3b72377d7e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_dev = 'duplicated_dev.txt'\n",
    "dup_train = 'duplicated_train.txt'\n",
    "dup_test = 'cleaned_dup_combined_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "610ca503-63b3-4e59-8c62-28cd4f3cf4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\n",
    "        \"chapter_name\",\n",
    "        \"sentence_number\",\n",
    "        \"token_number\",\n",
    "        \"word\",\n",
    "        \"lemma\",\n",
    "        \"part-of-speech\",\n",
    "        \"syntax\",\n",
    "        \"cue\",\n",
    "        \"scope\",\n",
    "        \"event\",\n",
    "    ]\n",
    "\n",
    "dev_df = pd.read_csv(\n",
    "        dup_dev,\n",
    "        sep=\"\\t\",\n",
    "        names=column_names,\n",
    "        encoding=\"utf-8\",\n",
    "        na_filter=False,\n",
    "    )\n",
    "\n",
    "train_df = pd.read_csv(\n",
    "        dup_train,\n",
    "        sep=\"\\t\",\n",
    "        names=column_names,\n",
    "        encoding=\"utf-8\",\n",
    "        na_filter=False,\n",
    "    )\n",
    "\n",
    "test_df = pd.read_csv(\n",
    "        dup_test,\n",
    "        sep=\"\\t\",\n",
    "        names=column_names,\n",
    "        encoding=\"utf-8\",\n",
    "        na_filter=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ac44d9-6d05-4df0-b9b8-e7d0bd2d829c",
   "metadata": {},
   "source": [
    "## Converting DF into a readable form for BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc01f67b-1d63-42bc-95cd-4da8d8c3eaf5",
   "metadata": {},
   "source": [
    "**Question:\n",
    "How BERT can be used for this particular type of NLP task and  the needs to process the input given the task**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43519fcf-5c5f-441f-9d6e-5b0c23106af4",
   "metadata": {},
   "source": [
    "Answer: BERT, as a pre-trained transformer model, is suitable for negation scope detection task, where the goal is to identify the text scope influenced by each negation cue. Through its transformer structure, BERT is able to identify contextual relationships in text, which is crucial for negation scope detection, as the scope of negation often depends on the relationship between a negation cue and the tokens it affects. Moreover, BERT's is bidirectional, allowing it to consider both preceding and following tokens when determining the scope of negation. This ability to identify rich, contextual dependencies makes BERT a powerful tool for negation scope detection tasks.\n",
    "\n",
    "We need to implement input processing to align with BERT's structure and tokenization method. Tokenized sentences are first further tokenized into subwords using BERT's tokenizer, ensuring that the input aligns with the model's fixed vocabulary. Special care is taken to map subwords back to their original tokens and align the associated labels (e.g. indicating whether a token is in or out of the negation scope). Labels for ignored tokens (e.g. special tokens or padding) are set to -100, ensuring that these tokens are excluded from training loss calculations.\n",
    "\n",
    "Moreover, preprocessing involves inserting custom tokens to mark negation cues explicitly, such as [NEG], [PRE], or [POST] to differentiate the types of negation patterns (e.g. single-word negation, prefix, or suffix cues). Assigning different negation cue type tags helps BERT improve its contextual understanding to learn nuanced relationships between tokens and their negation scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c67d787c-2314-401e-9486-122f26fe8cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df(df):\n",
    "    \"\"\"\n",
    "    Converts a DataFrame containing negation annotations into a list of dictionaries suitable for BERT.\n",
    "\n",
    "    Parameters:\n",
    "    df : pandas.DataFrame\n",
    "        Input DataFrame with the following required columns:\n",
    "        - chapter_name: Name of the chapter\n",
    "        - sentence_number: Sequential number identifying each sentence\n",
    "        - word: Individual tokens/words in the sentence\n",
    "        - scope: Scope annotations ('_' for out-scope, other values for in-scope)\n",
    "        - cue: Cue annotations ('_' for no cue, other values for cue)\n",
    "\n",
    "    Returns:\n",
    "    list of dict\n",
    "        List where each dictionary represents a sentence with the following keys:\n",
    "        - id: String identifier \n",
    "        - tokens: List of words in the sentence\n",
    "        - labels: Binary list indicating scope (0 for out-scope, 1 for in-scope)\n",
    "        - cues: Binary list indicating cues (0 for no cue, 1 for cue)\n",
    "        - og_cues: Original cue annotations before binarization\n",
    "        \n",
    "    Notes:\n",
    "    - The function creates a unique sentence identifier by combining chapter_name \n",
    "      and sentence_number\n",
    "    - Scope and cue values are binarized: '_' becomes 0, any other value becomes 1\n",
    "    - Original cue values are preserved in 'og_cues' for reference\n",
    "    \"\"\"\n",
    "\n",
    "        \n",
    "    # Combine chapter_name and sentence_number into a unique sentence ID\n",
    "    df['sentence_id'] = df['chapter_name'].astype(str) + '_' + df['sentence_number'].astype(str)\n",
    "    \n",
    "    \n",
    "    # Group by sentence ID and construct the dictionaries\n",
    "    result = [\n",
    "        {\n",
    "            'id': str(i),\n",
    "            'tokens': group['word'].tolist(),\n",
    "            'labels': [0 if item == '_' else 1 for item in group['scope']] ,\n",
    "            'cues': [0 if val == '_' else 1 for val in group['cue']],\n",
    "            'og_cues': group['cue'].tolist()\n",
    "        }\n",
    "        for i, (_, group) in enumerate(df.groupby('sentence_id',sort=False))\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3496571e-fc00-4d30-9e15-f6ced07ac943",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = convert_df(dev_df)\n",
    "train_data = convert_df(train_df)\n",
    "test_data = convert_df(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f5e6150b-78fb-42f0-a3af-0f0e4079936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(data):\n",
    "    \"\"\"\n",
    "    Augments input data by processing negation cues and adding special tokens to mark different types\n",
    "    of negation patterns in the text.\n",
    "    \n",
    "    Parameters:\n",
    "    data : list[dict]\n",
    "        A list of dictionaries, where each dictionary contains:\n",
    "        - 'id': str: order identifier\n",
    "        - 'tokens': list[str]: The original tokens/words in the sentence\n",
    "        - 'labels': list[int]: Binary indicators (0/1) marking whether each token is part of a negation scope\n",
    "        - 'cues': list[int]: Binary indicators (0/1) marking whether each token is part of a negation cue\n",
    "        - 'og_cues': list[str]:The original negation cues corresponding to each token\n",
    "        \n",
    "    Returns:\n",
    "    list[dict]\n",
    "        Processed data with the almost same structure as input ('id' removed), but with:\n",
    "        - Additional special tokens inserted before negation cues:\n",
    "            [MULTI]: For tokens that are part of multiple negation cues\n",
    "            [NEG]: For single negation cues that is single-word negation cues\n",
    "            [PRE]: For single negation cues where original cue is a prefix\n",
    "            [POST]: For single negation cues where original cue is a suffix\n",
    "        - Updated labels: -100 for inserted special tokens\n",
    "        - Preserved cues and original cues lists with appropriate padding\n",
    "        \n",
    "    Notes:\n",
    "    ------\n",
    "    - The function preserves the original token sequence while adding special markers\n",
    "    - Labels for inserted special tokens are set to -100 (typically used to ignore these\n",
    "      tokens during loss calculation in machine learning tasks)\n",
    "    - The function handles three types of negation patterns:\n",
    "        1. Multiple negation cues in the same sentence\n",
    "        2. Single negation cues that match exactly\n",
    "        3. Single negation cues that are part of larger tokens (suffix or within)\n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    final =[]\n",
    "    for sentence in data:\n",
    "        \n",
    "        # Extract relevant lists\n",
    "        tokens = sentence['tokens']\n",
    "        cues = sentence['cues']\n",
    "        og_cues = sentence['og_cues']\n",
    "        labels = sentence['labels']\n",
    "    \n",
    "        # Count the number of 1s in the cues -- 1 indicates the cue\n",
    "        cue_count = cues.count(1)\n",
    "    \n",
    "        # Process tokens based on the cues\n",
    "        processed_tokens = []\n",
    "        processed_labels = []\n",
    "        processed_cues = []\n",
    "        processed_og_cues = []\n",
    "        for idx, (token, cue) in enumerate(zip(tokens, cues)):\n",
    "            if cue == 1:\n",
    "                processed_labels.append(-100) # if cue == 1 (the particular token is a cue), it will be recognized as a special token\n",
    "                processed_cues.append(cues[idx])\n",
    "                processed_og_cues.append(og_cues[idx])\n",
    "                # Multiple cues: mark as [MULTI]\n",
    "                if cue_count > 1: # if number of cues is bigger than 1 in the sentence, add [MULTI] before the first cue token\n",
    "                    processed_tokens.append(\"[MULTI]\")\n",
    "                # Single cue: mark based on position\n",
    "                elif og_cues[idx] == token: # if the cue is exactly the token in the words column, add [NEG] before the cue token\n",
    "                    processed_tokens.append(\"[NEG]\")  \n",
    "                else:\n",
    "                    if token.startswith(og_cues[idx]):\n",
    "                        processed_tokens.append(\"[PRE]\")\n",
    "                    else:\n",
    "                        processed_tokens.append(\"[POST]\")\n",
    "            processed_tokens.append(token)\n",
    "            processed_labels.append(labels[idx])\n",
    "            processed_cues.append(cues[idx])\n",
    "            processed_og_cues.append(og_cues[idx])\n",
    "    \n",
    "        final.append({\n",
    "            'tokens':processed_tokens,\n",
    "            'labels': processed_labels,\n",
    "            'cues': processed_cues,\n",
    "            'og_cues': processed_og_cues\n",
    "        })\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f790db93-66f2-470b-85c8-8de47d893adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug = augment_data(train_data)\n",
    "dev_aug =augment_data(dev_data)\n",
    "test_aug =augment_data(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3955ee47-78f2-4b80-8ce4-131d7a402a83",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a76c7-834b-4e3f-86ea-d589c686426e",
   "metadata": {
    "id": "YVx71GdAIrJH"
   },
   "source": [
    "Before we can feed those texts to our model, we need to preprocess them. This is done by a ðŸ¤— Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
    "\n",
    "To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n",
    "\n",
    "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
    "- we download the vocabulary used when pretraining this specific checkpoint.\n",
    "\n",
    "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4b9fe3d7-c6b9-4aab-9cf9-e0e2b5250809",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2fe68668-c17a-436e-91ca-b4c0459d4f49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5hBlsrHIrJL",
    "outputId": "01450c01-103a-47dc-e7b7-493d5b3762ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Four special tokens have been added to the tokenizer vocabulary\n",
    "tokenizer.add_tokens(['[PRE]', '[POST]', '[MULTI]', '[NEG]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54dc6da-12e2-48b4-b73c-aef22a60830a",
   "metadata": {
    "id": "WaKVIR4Habdf"
   },
   "source": [
    "Se need to do some processing on our labels as the input ids returned by the tokenizer are longer than the lists of labels our dataset contain, first because some special tokens might be added (for example, `[CLS]` and a `[SEP]`) and then because of possible splits of words in multiple tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a21ed26-d8cd-444a-94cd-92baff1022fa",
   "metadata": {
    "id": "0VuZyE4babdf"
   },
   "source": [
    "Thankfully, the tokenizer returns outputs that have a `word_ids` method which can help us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8529363d-8333-42b3-9549-dada9037fe00",
   "metadata": {
    "id": "pdsadGs4abdf"
   },
   "source": [
    "The method `word_ids` returns a list with the same number of elements as our processed input ids, mapping special tokens to `None` and all other tokens to their respective word. This way, we can align the labels with the processed input ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2e93a34b-667a-402c-97b4-920f7f4a16f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenized_input(data):\n",
    "    \"\"\"\n",
    "    Processes a list of sentences by tokenizing them and aligning their labels with the tokenized output.\n",
    "    \n",
    "    Parameter:\n",
    "    data : list[dict]: the list of dictionaries converted and augmented.\n",
    "        \n",
    "    Make changes in each sentence dictionary in place by adding:\n",
    "    - labels (list[int]): aligned labels where -100 represents tokens that are special tokens \n",
    "    - input_ids (list[int]): token ids that mark their place in the pretrained vocabulary\n",
    "    - attention_mask (list[int]): 1 indicates that the token is real and should be attended to. 0 indicates that the token is padding \n",
    "    and should be ignored.\n",
    "    - word_ids (list[int]): mapping from token positions to word positions\n",
    "    - tokenized_tokens (list[str]): The subtokens after tokenization\n",
    "            \n",
    "\n",
    "    Notes:\n",
    "        - Uses is_split_into_words=True as input is already tokenized\n",
    "        - label value -100 is used for tokens that should be ignored in loss computation\n",
    "    \"\"\"\n",
    "    tokenized_data=[]\n",
    "    for sent in data:\n",
    "        single_sent=sent.copy()  # copy the sentence to avoid modifying the original\n",
    "        tokenized_input = tokenizer(sent[\"tokens\"], is_split_into_words=True) # tokenize the sentence into subwords\n",
    "        tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"]) # convert subword token ids (vocabulary positions) to subword tokens\n",
    "        word_ids = tokenized_input.word_ids() # subword word ids\n",
    "        aligned_labels = [-100 if i is None else sent[\"labels\"][i] for i in word_ids]\n",
    "        \n",
    "        single_sent[\"labels\"] = aligned_labels\n",
    "        single_sent[\"input_ids\"] = tokenized_input[\"input_ids\"] # subword token ids\n",
    "        single_sent[\"attention_mask\"] = tokenized_input[\"attention_mask\"]\n",
    "        single_sent['word_ids'] = word_ids # subword word ids\n",
    "        single_sent['tokenized_tokens'] = tokens # subword tokens\n",
    "        tokenized_data.append(single_sent)\n",
    "    return tokenized_data\n",
    "\n",
    "train_aug = tokenized_input(train_aug)\n",
    "dev_aug = tokenized_input(dev_aug)\n",
    "test_aug = tokenized_input(test_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "edbf2623-49b9-4e77-b2d6-5775a7aa4aa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_example(example):\n",
    "    \"\"\"\n",
    "    Preprocesses input data by extracting relevant features for model input.\n",
    "    \n",
    "    This function takes a dictionary containing various features of a training example\n",
    "    and returns a new dictionary with only the essential features needed for model training:\n",
    "    input ids, attention mask, and labels.\n",
    "    \n",
    "    Argument:\n",
    "        example (dict): a dictionary containing the example's features, including at minimum:\n",
    "            - input_ids: ids marking the token's position in pretrained vocabulary\n",
    "            - attention_mask: binary mask indicating valid input tokens\n",
    "            - labels: aligned labels where -100 represents tokens that are special tokens, 1 are tokens within scope, 0 are tokens\n",
    "            out of scope\n",
    "            \n",
    "    Returns:\n",
    "        dict: a preprocessed dictionary containing:\n",
    "            - input_ids\n",
    "            - attention_mask\n",
    "            - labels\n",
    "            \n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"input_ids\": example[\"input_ids\"],\n",
    "        \"attention_mask\": example[\"attention_mask\"],\n",
    "        \"labels\": example[\"labels\"]\n",
    "    }\n",
    "\n",
    "# Preprocess the dataset\n",
    "processed_train = [preprocess_example(example) for example in train_aug]\n",
    "processed_dev = [preprocess_example(example) for example in dev_aug]\n",
    "processed_test = [preprocess_example(example) for example in test_aug]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab512e3e-4b5f-45b2-9150-7ea90ede15ce",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436be3a3-2292-4ec7-b6fc-8091c6f22a7c",
   "metadata": {
    "id": "FBiW8UpKIrJW"
   },
   "source": [
    "Now that our data is ready, we can download the pretrained model and fine-tune it. Since all our tasks are about token classification, we use the `AutoModelForTokenClassification` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us. The only thing we have to specify is the number of labels for our problem (which we can get from the features, as seen before):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bcc40f43-248d-4976-8550-bf6d0b47be2e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "referenced_widgets": [
      "6d55c83b8b914aa88e1ce4f767a89917",
      "56cadd26245e4a3c909a665f80b04611",
      "623a98fafb894a31b9f7e6ea436da09d",
      "5de0bf631b1c4eddb5c377e9af6285de",
      "bc0cfde945cd43ccade17fd44c38c353",
      "711cf9c2988a4cfa92515480310731ee",
      "23c6fb5d807046b0803182cc9c6dc229",
      "554069d9065a483fafabf13fa1354777",
      "00f8414756364ff0904386ddc62e8ab7",
      "1e70d6912b7a4350a1cc8f479f717e98",
      "1b9e96610f664d508b28b06e2f658f65"
     ]
    },
    "id": "TlqNaB8jIrJW",
    "outputId": "fa1aaa2d-3e59-4ba5-9799-47aaa4fb0ba5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30526, 768, padding_idx=0)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=2) \n",
    "# resize the model's token embeddings to include the new tokens\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc05b2c-5253-4d2c-9228-198e2db57c76",
   "metadata": {
    "id": "CczA5lJlIrJX"
   },
   "source": [
    "The warning is telling us we are throwing away some weights (the `vocab_transform` and `vocab_layer_norm` layers) and randomly initializing some other (the `pre_classifier` and `classifier` layers). This is absolutely normal in this case, because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73105ef-37c5-4e11-9dd1-c6a214d8326c",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "To instantiate a `Trainer`, we will need to define three more things. The most important is the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:\n",
    "\n",
    "In this assignment, you will experiment with different random seeds and see what the outcome is on the performance of the model. The random seed influences the following steps when training (amongst others):\n",
    "\n",
    "\n",
    "*  **Weight Initialization:** When we start training, the values for the weights are randomly assigned. This is controlled by the random seed. This impacts the training trajectory in the weight space, e.g. determines which local minimum the model can end up in.\n",
    "*  **Data Order**: The order in which the model sees the data is also controlled by the random seed. This also influences the training trajectory, determining which data points are e.g. influential in the beginning.\n",
    "\n",
    "\n",
    "Setting the random seed with a specific numerical value (e.g. 0) makes sure that we can reproduce the results again, introducing some kind of deterministic behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3da87b40-a5dc-4651-b7d6-b8f75bd692c4",
   "metadata": {
    "id": "Bliy8zgjIrJY"
   },
   "outputs": [],
   "source": [
    "# Set random seed!\n",
    "SEED = 99\n",
    "set_seed(SEED)\n",
    "\n",
    "model_name = model_checkpoint\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",# The output directory name\n",
    "    eval_strategy = \"epoch\", # Evaluation will be performed at the end of each epoch\n",
    "    learning_rate=2e-5, # Learning rate of 0.00002 (2e-5) - a common value for transformer fine-tuning\n",
    "    per_device_train_batch_size=batch_size,# Number of training examples processed in parallel per device during training\n",
    "    per_device_eval_batch_size=batch_size,# Number of evaluation examples processed in parallel per device during evaluation\n",
    "    num_train_epochs=3,# Total number of training epochs (complete passes through the dataset)\n",
    "    weight_decay=0.01,# Weight decay of 0.01 for regularization to prevent overfitting\n",
    "    seed=SEED,\n",
    "    report_to=None,# Disables reporting to external tracking services\n",
    "    logging_dir='./logs', # Directory where training logs will be saved        \n",
    "    logging_steps=10, # Log training metrics every 10 steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330b92d9-b6b7-4a11-8910-5381c2e50820",
   "metadata": {
    "id": "NaH-uijhabdi"
   },
   "source": [
    "Then we will need a data collator that will batch our processed examples together while applying padding to make them all the same size (each pad will be padded to the length of its longest example). There is a data collator for this task in the Transformers library, that not only pads the inputs, but also the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f8f1f281-9ad2-44da-b469-b4d47ab22a92",
   "metadata": {
    "id": "16HZ2oCMabdi"
   },
   "outputs": [],
   "source": [
    "max_len = max(max([len(sent['input_ids']) for sent in processed_train]) , max([len(sent['input_ids']) for sent in processed_dev]) ,max([len(sent['input_ids']) for sent in processed_test]))\n",
    "\n",
    "# batch our processed examples together while applying padding to make them all the same size \n",
    "# (each pad will be padded to the length of its longest example)\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    padding='max_length',\n",
    "    max_length=max_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54de589-8b6e-47fc-b36d-1b1f0a2cfd13",
   "metadata": {
    "id": "NaH-uijhabdi"
   },
   "source": [
    "Then we will need a data collator that will batch our processed examples together while applying padding to make them all the same size (each pad will be padded to the length of its longest example). There is a data collator for this task in the Transformers library, that not only pads the inputs, but also the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fcf4b62a-8d43-4899-a5e9-cb6e43701e17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "6b641a7b761d4441a512e6a123b7b1da",
      "9bbd71548a0742619af880dd2e12d53d",
      "b98a50c41c204391944a4a6a1ab8a009",
      "3368f0ae8f6140328ed5b7f9533a40c7",
      "be212c97c9024812a47bd3c08c54ec6a",
      "3ae3cfe031da4af496ad561d5a298553",
      "c1c40fb068f54cebbb3155c2ab2c7ccd",
      "d4925754502c4cf1a6ff0d55fa977cd0",
      "481d862da3de4e8a97d07c938c6678a2",
      "865e4bf0754b4f85b813e5179d52c79e",
      "41b5a90a9aed40b18764599d0de0f41d"
     ]
    },
    "id": "s1TNz926abdi",
    "outputId": "653d3dd0-5649-4d31-a189-f6f059381b92"
   },
   "outputs": [],
   "source": [
    "metric = load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c19d82-2288-497e-885f-152b9b510e8c",
   "metadata": {
    "id": "7sZOdRlRIrJd"
   },
   "source": [
    "We will need to do a bit of post-processing on our predictions:\n",
    "- select the predicted index (with the maximum logit) for each token\n",
    "- convert it to its string label\n",
    "- ignore everywhere we set a label of -100\n",
    "\n",
    "The following function does all this post-processing on the result of `Trainer.evaluate` (which is a named tuple containing predictions and labels) before applying the metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4edef668-0713-4f65-b45b-7445d9fc79cf",
   "metadata": {
    "id": "UmvbnJ9JIrJd"
   },
   "outputs": [],
   "source": [
    "label_list = ['OUT' , 'IN']\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for token classification predictions.\n",
    "\n",
    "    This function calculates precision, recall, f1 score, and accuracy for token-level\n",
    "    predictions. It handles special tokens by removing ignored indices (-100) before computation.\n",
    "\n",
    "    Argument:\n",
    "        p (tuple): A tuple containing:\n",
    "            - predictions (numpy.ndarray): model predictions with shape (batch_size, sequence_length, num_labels)\n",
    "            - labels (numpy.ndarray): ground truth labels with shape (batch_size, sequence_length)\n",
    "                Special tokens are marked with -100 and will be ignored in evaluation\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the following metrics:\n",
    "            - precision (float): overall precision across all classes\n",
    "            - recall (float): overall recall across all classes\n",
    "            - f1 (float): overall f1 score across all classes\n",
    "            - accuracy (float): overall accuracy across all classes\n",
    "\n",
    "    Notes:\n",
    "        - The function expects label_list to be defined in the outer scope\n",
    "        - The metric object should be defined in the outer scope and support the\n",
    "          compute() method with predictions and references parameters\n",
    "        - Predictions are converted from logits to label indices using argmax\n",
    "        - Special tokens (labeled as -100) are removed before metric computation\n",
    "    \"\"\"\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "630e28d9-c5ea-4670-972f-4f6ea527d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(processed_doc):\n",
    "    \"\"\"\n",
    "\n",
    "    1. Extracts key components from the input data (input ids, attention masks, labels).\n",
    "    2. Pack these components into a Hugging Face Dataset object.\n",
    "    3. The Dataset object is specifically structured for use with the Hugging Face Trainer.\n",
    "\n",
    "    Argument:\n",
    "        processed_doc (list[dict]): a list of dictionaries, where each dictionary represents\n",
    "                                    a processed sentence. Each dictionary must include:\n",
    "                                    - 'input_ids': token ids for the sentence.\n",
    "                                    - 'attention_mask': to distinguish real tokens from padding (1 for real tokens, 0 for padding).\n",
    "                                    - 'labels': (sub)token-level integer labels for classification.\n",
    "\n",
    "    Return:\n",
    "        Dataset: a Hugging Face Dataset object containing:\n",
    "                 - 'input_ids'\n",
    "                 - 'attention_mask'\n",
    "                 - 'labels'\n",
    "    \"\"\"\n",
    "    final_dataset = Dataset.from_dict({\n",
    "        'input_ids': [example['input_ids'] for example in processed_doc],\n",
    "        'attention_mask': [example['attention_mask'] for example in processed_doc],\n",
    "        'labels': [example['labels'] for example in processed_doc]\n",
    "    })\n",
    "    return final_dataset\n",
    "\n",
    "# Convert processed_data (list of dicts) into a Dataset\n",
    "train_dataset = make_dataset(processed_train)\n",
    "dev_dataset = make_dataset(processed_dev)\n",
    "test_dataset = make_dataset(processed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac8860-cee7-4e9c-b3e4-c0b079f04eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5947c3ea-d460-4914-847b-0a32fc524eb9",
   "metadata": {},
   "source": [
    "**A printed example showing an excerpt (1 or 2 sentences) of the data as it is fed into the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "42b1aa92-2667-43e1-a9b5-e1d496c7ad7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101,\n",
       "   2720,\n",
       "   1012,\n",
       "   20052,\n",
       "   9106,\n",
       "   1010,\n",
       "   2040,\n",
       "   2001,\n",
       "   2788,\n",
       "   2200,\n",
       "   2397,\n",
       "   1999,\n",
       "   1996,\n",
       "   16956,\n",
       "   1010,\n",
       "   3828,\n",
       "   2588,\n",
       "   2216,\n",
       "   2025,\n",
       "   30522,\n",
       "   1999,\n",
       "   19699,\n",
       "   2063,\n",
       "   15417,\n",
       "   6642,\n",
       "   2043,\n",
       "   2002,\n",
       "   2001,\n",
       "   2039,\n",
       "   2035,\n",
       "   2305,\n",
       "   1010,\n",
       "   2001,\n",
       "   8901,\n",
       "   2012,\n",
       "   1996,\n",
       "   6350,\n",
       "   2795,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   2720,\n",
       "   1012,\n",
       "   20052,\n",
       "   9106,\n",
       "   1010,\n",
       "   2040,\n",
       "   2001,\n",
       "   2788,\n",
       "   2200,\n",
       "   2397,\n",
       "   1999,\n",
       "   1996,\n",
       "   16956,\n",
       "   1010,\n",
       "   3828,\n",
       "   2588,\n",
       "   2216,\n",
       "   30525,\n",
       "   2025,\n",
       "   1999,\n",
       "   19699,\n",
       "   2063,\n",
       "   15417,\n",
       "   6642,\n",
       "   2043,\n",
       "   2002,\n",
       "   2001,\n",
       "   2039,\n",
       "   2035,\n",
       "   2305,\n",
       "   1010,\n",
       "   2001,\n",
       "   8901,\n",
       "   2012,\n",
       "   1996,\n",
       "   6350,\n",
       "   2795,\n",
       "   1012,\n",
       "   102]],\n",
       " 'attention_mask': [[1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1]],\n",
       " 'labels': [[-100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   -100,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100],\n",
       "  [-100,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   -100,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   -100]]}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a8b0eb49-1545-413b-bc74-9538722b85e2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['Since',\n",
       "  'we',\n",
       "  'have',\n",
       "  'been',\n",
       "  'so',\n",
       "  'unfortunate',\n",
       "  'as',\n",
       "  'to',\n",
       "  'miss',\n",
       "  'him',\n",
       "  'and',\n",
       "  'have',\n",
       "  '[NEG]',\n",
       "  'no',\n",
       "  'notion',\n",
       "  'of',\n",
       "  'his',\n",
       "  'errand',\n",
       "  ',',\n",
       "  'this',\n",
       "  'accidental',\n",
       "  'souvenir',\n",
       "  'becomes',\n",
       "  'of',\n",
       "  'importance',\n",
       "  '.'],\n",
       " 'labels': [-100,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  -100,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100],\n",
       " 'cues': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'og_cues': ['_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  'no',\n",
       "  'no',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_'],\n",
       " 'input_ids': [101,\n",
       "  2144,\n",
       "  2057,\n",
       "  2031,\n",
       "  2042,\n",
       "  2061,\n",
       "  15140,\n",
       "  2004,\n",
       "  2000,\n",
       "  3335,\n",
       "  2032,\n",
       "  1998,\n",
       "  2031,\n",
       "  30525,\n",
       "  2053,\n",
       "  9366,\n",
       "  1997,\n",
       "  2010,\n",
       "  9413,\n",
       "  13033,\n",
       "  1010,\n",
       "  2023,\n",
       "  17128,\n",
       "  2061,\n",
       "  27346,\n",
       "  4313,\n",
       "  4150,\n",
       "  1997,\n",
       "  5197,\n",
       "  1012,\n",
       "  102],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'word_ids': [None,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  21,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  None],\n",
       " 'tokenized_tokens': ['[CLS]',\n",
       "  'since',\n",
       "  'we',\n",
       "  'have',\n",
       "  'been',\n",
       "  'so',\n",
       "  'unfortunate',\n",
       "  'as',\n",
       "  'to',\n",
       "  'miss',\n",
       "  'him',\n",
       "  'and',\n",
       "  'have',\n",
       "  '[NEG]',\n",
       "  'no',\n",
       "  'notion',\n",
       "  'of',\n",
       "  'his',\n",
       "  'er',\n",
       "  '##rand',\n",
       "  ',',\n",
       "  'this',\n",
       "  'accidental',\n",
       "  'so',\n",
       "  '##uven',\n",
       "  '##ir',\n",
       "  'becomes',\n",
       "  'of',\n",
       "  'importance',\n",
       "  '.',\n",
       "  '[SEP]']}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_aug[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f7b97d-8655-42dc-8a6b-9a2324c76d96",
   "metadata": {},
   "source": [
    "**Question:\n",
    "Explain and exemplify, providing 1 or 2 examples, how you chose to preprocess\r\n",
    "the input. Make sure your examples include both human-readable (i.e., using text)\r\n",
    "examples and machine-readable input (i.e., using sub-word ids**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05653538-587c-4811-8214-c2f5bb838ffc",
   "metadata": {},
   "source": [
    "Answer:\n",
    "First, the raw dataframe, which contains columns of tokens, cues, and scopes, is converted into a list of dictionaries, where each dictionary represents information of a sentence. Each dictionary contains fields including 'tokens', 'labels' (binary labels for scope, 0 for out-of-scope and 1 for in-scope), 'cues' (binary labels for cues, 0 for no cue and 1 for a cue), and 'og_cues' (the original cue annotations in strings).\n",
    "\n",
    "After that, an augmentation step is applied to the field 'tokens' and 'labels'. Tokens associated with negation cues are marked with special tokens such as [NEG], [PRE], [POST], or [MULTI], depending on the types of negation cue. This additional marking helps the model to refer to the context around negation cues. Special tokens inserted during this step are ignored during loss calculations by setting their labels to -100. The updated data (list of dictionaries) with the same field names.\n",
    "\n",
    "The data is then tokenized using the AutoTokenizer. This tokenizer converts words into subwords, and provide input ids based on the model's pretrained vocabulary, and at the same time, negation type tokens are updated as special tokens. Subwords and their original tokens are mapped using the word_ids method of the tokenizer. Word ids set -100 for special tokens. A new list of dictionaries is created with fields including 'labels' (binary labels for scope), 'input_ids' (pretrained vocabulary ids), 'attention_mask' (a binary mask for distinguishing real tokens from padding), 'word_ids' (sequntial ids mapping subwords and their original tokens) and 'tokenized_tokens' (tokenized tokens).\n",
    "\n",
    "Example (see the output of train_aug[5]):\n",
    "'tokenized_tokens': ['[CLS]',\n",
    "  'since',\n",
    "  'we',\n",
    "  'have',\n",
    "  'been',\n",
    "  'so',\n",
    "  'unfortunate',\n",
    "  'as',\n",
    "  'to',\n",
    "  'miss',\n",
    "  'him',\n",
    "  'and',\n",
    "  'have',\n",
    "  '[NEG]',\n",
    "  'no',\n",
    "  'notion',\n",
    "  'of',\n",
    "  'his',\n",
    "  'er',\n",
    "  '##rand',\n",
    "  ',',\n",
    "  'this',\n",
    "  'accidental',\n",
    "  'so',\n",
    "  '##uven',\n",
    "  '##ir',\n",
    "  'becomes',\n",
    "  'of',\n",
    "  'importance',\n",
    "  '.',\n",
    "  '[SEP]'],\n",
    "\n",
    "'input_ids': [101,\n",
    "  2144,\n",
    "  2057,\n",
    "  2031,\n",
    "  2042,\n",
    "  2061,\n",
    "  15140,\n",
    "  2004,\n",
    "  2000,\n",
    "  3335,\n",
    "  2032,\n",
    "  1998,\n",
    "  2031,\n",
    "  30525,\n",
    "  2053,\n",
    "  9366,\n",
    "  1997,\n",
    "  2010,\n",
    "  9413,\n",
    "  13033,\n",
    "  1010,\n",
    "  2023,\n",
    "  17128,\n",
    "  2061,\n",
    "  27346,\n",
    "  4313,\n",
    "  4150,\n",
    "  1997,\n",
    "  5197,\n",
    "  1012,\n",
    "  102],\n",
    "\n",
    "'word_ids': [None,\n",
    "  0,\n",
    "  1,\n",
    "  2,\n",
    "  3,\n",
    "  4,\n",
    "  5,\n",
    "  6,\n",
    "  7,\n",
    "  8,\n",
    "  9,\n",
    "  10,\n",
    "  11,\n",
    "  12,\n",
    "  13,\n",
    "  14,\n",
    "  15,\n",
    "  16,\n",
    "  17,\n",
    "  17,\n",
    "  18,\n",
    "  19,\n",
    "  20,\n",
    "  21,\n",
    "  21,\n",
    "  21,\n",
    "  22,\n",
    "  23,\n",
    "  24,\n",
    "  25,\n",
    "  None]\n",
    "\n",
    "In this example, we can see that the tokenized tokens include words split into subwords (e.g. 'souvenir' becomes 'so', '##uven', '##ir'), and each subword token corresponds to an input id. However, the original word can still be tracked using the word ids, since same words are given identical word ids (e.g., 'so', '##uven', '##ir' are all assigned with word id 21).\n",
    "\n",
    "Finally, the tokenized inputs are prepared for the model by extracting fields from the new list of dictionaries. These fields include 'input_ids', 'attention_mask', 'labels'. The processed data is then converted into a Hugging Face Dataset object for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4d107b-669e-4c7b-afdb-224fda213357",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## Training and Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3953814d-f845-40e4-8799-25813058895c",
   "metadata": {
    "id": "rXuFTAzDIrJe"
   },
   "source": [
    "Note that we drop the precision/recall/f1 computed for each category and only focus on the overall precision/recall/f1/accuracy.\n",
    "\n",
    "Then we just need to pass all of this along with our datasets to the `Trainer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "976dfc43-3c30-4227-8eaf-1bbf0caa1699",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "imY1oC3SIrJf",
    "outputId": "0ac68999-9afd-4432-e5e4-54022e0f9e6a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13760\\AppData\\Local\\Temp\\ipykernel_9292\\2245732466.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args, # training arguments\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset, # validation data\n",
    "    data_collator=data_collator, # batching and padding\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics # evaluation function\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2362130b-82eb-4195-ba05-a97e1089e5e1",
   "metadata": {
    "id": "CdzABDVcIrJg"
   },
   "source": [
    "We can now finetune our model by just calling the `train` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5395e662-02aa-4648-8174-f4d26e16b688",
   "metadata": {
    "id": "4tWR58-Dabdj"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='186' max='186' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [186/186 10:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.348100</td>\n",
       "      <td>0.370463</td>\n",
       "      <td>0.426471</td>\n",
       "      <td>0.488215</td>\n",
       "      <td>0.455259</td>\n",
       "      <td>0.840444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.161500</td>\n",
       "      <td>0.308163</td>\n",
       "      <td>0.597101</td>\n",
       "      <td>0.693603</td>\n",
       "      <td>0.641745</td>\n",
       "      <td>0.885635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.168800</td>\n",
       "      <td>0.308104</td>\n",
       "      <td>0.623188</td>\n",
       "      <td>0.723906</td>\n",
       "      <td>0.669782</td>\n",
       "      <td>0.894219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13760\\AppData\\Roaming\\Python\\Python312\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: IN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\13760\\AppData\\Roaming\\Python\\Python312\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: OUT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\13760\\AppData\\Roaming\\Python\\Python312\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: IN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\13760\\AppData\\Roaming\\Python\\Python312\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: OUT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=186, training_loss=0.26089123404154213, metrics={'train_runtime': 631.723, 'train_samples_per_second': 4.673, 'train_steps_per_second': 0.294, 'total_flos': 70809897307968.0, 'train_loss': 0.26089123404154213, 'epoch': 3.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start fine-tuning the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6dcec6eb-a73a-4b62-b633-6390053353b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use the trained model to make predictions on the test dataset\n",
    "# 'predictions' are logits, which are unnormalized scores outputted by the model for each class ('OUT', 'IN'). \n",
    "# 'labels': true labels for the tokens in the test dataset.\n",
    "# axis=2: find the index of the maximum value along the last dimension (class scores for each token). In this example:\n",
    "\n",
    "# e.g. for one of the sentence:\n",
    "\n",
    "# Token 1: [2.5, -1.3] â†’ Class 0 (e.g. 'IN') (highest score: 2.5).\n",
    "# Token 2: [0.8, 1.9] â†’ Class 1 (e.g. 'OUT') (highest score: 1.9).\n",
    "\n",
    "predictions, labels, _ = trainer.predict(test_dataset)\n",
    "predictions = np.argmax(predictions, axis=2) # convert logits into actual class predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64994fda-ebf5-4bb0-aef7-ae6c439c0784",
   "metadata": {
    "id": "CKASz-2vIrJi"
   },
   "source": [
    "The `evaluate` method allows you to evaluate again on the evaluation dataset or on another dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c11dd33-3e13-49d7-a203-407a4d7054c0",
   "metadata": {
    "id": "UOUcBkX8IrJi"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3081037104129791,\n",
       " 'eval_precision': 0.6231884057971014,\n",
       " 'eval_recall': 0.7239057239057239,\n",
       " 'eval_f1': 0.6697819314641744,\n",
       " 'eval_accuracy': 0.8942186316586721,\n",
       " 'eval_runtime': 9.2703,\n",
       " 'eval_samples_per_second': 18.662,\n",
       " 'eval_steps_per_second': 1.187,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19790e7a-7933-4ebe-8600-512c461b34fb",
   "metadata": {},
   "source": [
    "## Processing of the Predictions and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "570926fe-0c6e-4ded-a791-1b207899a562",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, i in enumerate(test_aug):\n",
    "    # get the number of tokens (words) in the current sentence\n",
    "    length = len(i['word_ids'])\n",
    "    \n",
    "    # assign the model's predicted labels for this sentence\n",
    "    # slice the predictions to match the actual length of the sentence (ignoring padding tokens)\n",
    "    i['predictions'] = predictions[idx][:length]\n",
    "    i['gold'] = labels[idx][:length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5e7ba7e9-2ea1-40e2-b6a6-7e7f4b4c6e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_list):\n",
    "    \"\"\"\n",
    "    Takes a list of tokenized sentences and converts their token-level\n",
    "    predictions and gold labels into word-level predictions and gold labels using\n",
    "    majority voting. It removes unwanted special tokens like [CLS], [SEP], and padding tokens.\n",
    "\n",
    "    1. Filters out special tokens and their associated predictions/labels.\n",
    "    2. Groups subword tokens belonging to the same original word based on 'word_ids'.\n",
    "    3. Uses majority voting to assign a single prediction and gold label to each word.\n",
    "\n",
    "    Argument:\n",
    "        data_list (list[dict]): A list of dictionaries (aumented test data list), where each dictionary represents\n",
    "                                a tokenized sentence. Each dictionary must include:\n",
    "                                - 'word_ids': list mapping subwords to original words.\n",
    "                                - 'predictions': token-level predictions from the model.\n",
    "                                - 'gold': token-level gold (true) labels.\n",
    "                                - 'tokenized_tokens': the tokenized tokens (with subwords).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "               - processed_predictions_list (list[list[int]]): word-level predictions for all sentences.\n",
    "               - processed_gold_list (list[list[int]]): word-level gold labels for all sentences.\n",
    "    \"\"\"\n",
    "    def process_single(data):\n",
    "        word_ids = data['word_ids']\n",
    "        predictions = data['predictions']\n",
    "        gold = data['gold']\n",
    "        tokenized_tokens = data['tokenized_tokens']\n",
    "        # filter out special tokens like [CLS], [SEP], [PAD], etc\n",
    "        word_ids = [b for a, b in zip(tokenized_tokens, word_ids) if a not in ['[CLS]','[SEP]','[PAD]','[NEG]','[MULTI]','[PRE]','[POST]','[UNK]','[MASK]']]\n",
    "        predictions = [b for a, b in zip(tokenized_tokens, predictions) if a not in ['[CLS]','[SEP]','[PAD]','[NEG]','[MULTI]','[PRE]','[POST]','[UNK]','[MASK]']]\n",
    "        gold = [b for a, b in zip(tokenized_tokens, gold) if a not in ['[CLS]','[SEP]','[PAD]','[NEG]','[MULTI]','[PRE]','[POST]','[UNK]','[MASK]']]\n",
    "        \n",
    "        processed_predictions = [] # word-level predictions\n",
    "        processed_gold = []\n",
    "        \n",
    "        # Track the current word group\n",
    "        current_word_id = None\n",
    "        current_predictions = []\n",
    "        current_gold = []\n",
    "        \n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id != current_word_id:\n",
    "                # If a new word_id starts, calculate majority vote for the previous group\n",
    "                if current_predictions:\n",
    "                    # use majority voting to decide the final prediction/label for the word\n",
    "                    processed_predictions.append(Counter(current_predictions).most_common(1)[0][0])\n",
    "                    # most_common(1) returns the single most common element\n",
    "                    # 1st [0] extracts the first tuple from the list of most common elements. e.g. [(1, 2)] becomes (1, 2)\n",
    "                    # 2nd [0] extracts the first element of the tuple (the most common prediction itself). e.g. (1, 2) becomes 1\n",
    "                    processed_gold.append(Counter(current_gold).most_common(1)[0][0])\n",
    "                \n",
    "                # reset for the new word group\n",
    "                current_word_id = word_id\n",
    "                current_predictions = [predictions[idx]]\n",
    "                current_gold = [gold[idx]]\n",
    "            else:\n",
    "                # Add to the current group\n",
    "                current_predictions.append(predictions[idx])\n",
    "                current_gold.append(gold[idx])\n",
    "        \n",
    "        # deal with the last word group\n",
    "        if current_predictions:\n",
    "            processed_predictions.append(Counter(current_predictions).most_common(1)[0][0])\n",
    "            processed_gold.append(Counter(current_gold).most_common(1)[0][0])\n",
    "        \n",
    "        return processed_predictions, processed_gold\n",
    "\n",
    "    # lists to hold results for all sentences\n",
    "    processed_predictions_list = []\n",
    "    processed_gold_list = []\n",
    "    \n",
    "    # process each sentence in the dataset\n",
    "    for data in data_list:\n",
    "        processed_predictions, processed_gold = process_single(data)\n",
    "        processed_predictions_list.append(processed_predictions)\n",
    "        processed_gold_list.append(processed_gold)\n",
    "    \n",
    "    return processed_predictions_list, processed_gold_list\n",
    "\n",
    "\n",
    "processed_predictions, processed_gold = process_data(test_aug)\n",
    "\n",
    "# # Uncomment to view the results\n",
    "# # print(\"Processed Predictions List:\", processed_predictions)\n",
    "# # print(\"Processed Gold List:\", processed_gold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98b55bb-dc89-4d83-8bd3-eedb60e3bae3",
   "metadata": {},
   "source": [
    "**Question: Explain and exemplify, providing 1 or 2 examples, how you process the output of\r\n",
    "the model. Make sure that your systemâ€™s predictions match the tokenization required by\r\n",
    "the shared-task. Describe any heuristics you use to go from subword to token level\r\n",
    "predicti**ons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ff7b90-2e4f-4f1f-9ceb-0bfaa330a4a9",
   "metadata": {},
   "source": [
    "Answer: Since BERT tokenizes words into subwords, the predictions are first provided at the subword level, but the task requires word-level predictions. Therefore, processing of the predictions was conducted to ensure that subword predictions are mapped back to the original words.\n",
    "\n",
    "For each tokenized sentence, the subword-level predictions and gold labels are extracted from the model's output. The word_ids maps subwords back to their corresponding words and special tokens like [CLS], [SEP], and padding are excluded. The predictions and gold labels for subwords belonging to the same word are grouped based on their word ids.\n",
    "\n",
    "Within each word group, majority voting is applied to decide the final prediction and gold label for the word. For example, if a word split into tree subwords has predictions [0, 1, 0], the majority prediction 0 will be chosen as the final prediction for the word. This logic ensures a consistent word-level label and consider cases when the subword predictions differ.\n",
    "\n",
    "For example, a tokenized sentence with the subwords and their corresponding predictions:\n",
    "\n",
    "Subwords: ['dis', '##agree', '##ment']\n",
    "\n",
    "Word ids: [9, 9, 9]\n",
    "\n",
    "Predictions: [0, 1, 1]\n",
    "\n",
    "The subwords 'dis', '##agree' and '##ment' belong to the same word (word id: 9). The predictions for these subwords are [0, 1, 1]. Using majority voting, the final word-level prediction for 'disagreement' becomes 1.\n",
    "\n",
    "This process ensures that the final predictions is in word level required by the task and filtering out irrelevant tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16e36296-b79c-4c4c-99cb-a1a40d650ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(processed_predictions[0])\n",
    "# print(processed_gold[0])\n",
    "# print(test_aug[0])\n",
    "special_tokens = tokenizer.all_special_tokens\n",
    "special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7046e67b-8376-4323-93ee-11336db019e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = processed_gold\n",
    "pred = processed_predictions\n",
    "gold = [1 if item == 1 else 0 for sublist in gold for item in sublist]\n",
    "pred = [1 if item == 1 else 0 for sublist in pred for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3dbcc910-257d-4b9d-8d82-bf2af63ee8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_tsv(input_file, output_file, prediction, gold):\n",
    "    \"\"\"\n",
    "    Writes negation scope predictions to an output TSV file.\n",
    "\n",
    "    Parameters:\n",
    "    input_file : str\n",
    "        Path to input TSV file.\n",
    "    output_file : str\n",
    "        Path to output TSV file.\n",
    "    prediction : list of list of int\n",
    "        Predicted negation labels (1 for negation, 0 for non-negation).\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    outfile = open(output_file, \"w\")\n",
    "\n",
    "    # prediction\n",
    "    # flat_list = [token for sentence in prediction for token in sentence]\n",
    "\n",
    "    index = 0\n",
    "    for line in open(input_file, \"r\"):\n",
    "        if len(line.rstrip(\"\\n\").split()) > 0:\n",
    "            if prediction[index] == 1:\n",
    "                # this only works because index 3 is the base word\n",
    "                data = line.split()[3]\n",
    "            else:\n",
    "                data = \"_\"\n",
    "            if gold[index] == 1:\n",
    "                # this only works because index 3 is the base word\n",
    "                gold_d = line.split()[3]\n",
    "            else:\n",
    "                gold_d = \"_\"\n",
    "            outfile.write(line.rstrip(\"\\n\") + \"\\t\" + data + \"\\t\" + gold_d + \"\\n\")\n",
    "            index += 1\n",
    "\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09c730dc-8eb1-47d8-859d-151ea7c8432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tsv(dup_test,'./prediction.tsv',pred,gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e348a609-d5a4-40bd-a781-a524cfaa6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_file(tsv_file):\n",
    "    \"\"\"\n",
    "    Evaluate a TSV file with gold and predicted labels by creating a classification report and Confusion Matrix.\n",
    "\n",
    "    :param tsv_file: Path to the TSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    columns = [\n",
    "        \"chapter_name\",\n",
    "        \"sentence_number\",\n",
    "        \"token_number\",\n",
    "        \"word\",\n",
    "        \"lemma\",\n",
    "        \"part-of-speech\",\n",
    "        \"syntax\",\n",
    "        \"cue\",\n",
    "        \"scope\",\n",
    "        \"event\",\n",
    "        \"pred\",\n",
    "        \"gold\"\n",
    "    ]\n",
    "\n",
    "    df = pd.read_csv(tsv_file, sep=\"\\t\", header=None, names=columns)\n",
    "\n",
    "    y_true = df[\"scope\"].apply(lambda x: 0 if x == \"_\" else 1).tolist()\n",
    "    y_pred = df[\"pred\"].apply(lambda x: 0 if x == \"_\" else 1).tolist()\n",
    "\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=[\"Out of Scope\", \"In Scope\"], digits = 3\n",
    "    )\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "\n",
    "    target_names = [\"Out of Scope\", \"In Scope\"]\n",
    "    disp = ConfusionMatrixDisplay(\n",
    "        confusion_matrix=cm, display_labels=target_names\n",
    "    )\n",
    "    disp.plot(cmap=\"magma\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "02b86f81-2a46-4d8b-bd1b-41c1fb8fc504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Out of Scope      0.938     0.953     0.946      3884\n",
      "    In Scope      0.897     0.866     0.881      1826\n",
      "\n",
      "    accuracy                          0.925      5710\n",
      "   macro avg      0.917     0.910     0.913      5710\n",
      "weighted avg      0.925     0.925     0.925      5710\n",
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_file(\"prediction.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2989d77f-f152-4966-be13-3f89a429fd9d",
   "metadata": {},
   "source": [
    "**Model's preformance and comparison**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc403dde-2653-4953-8850-b0c06999b94e",
   "metadata": {},
   "source": [
    "My BERT model with a random seed of 99 shows strong overall performance in the task of token-level negation scope detection.The macro average f1 score is 0.913, and the weighted f1 score is 0.925, showing that the model performs consistently well across the dataset.\n",
    "\n",
    "My BERT model with a random seed of 99 shows a significant improvement over my previous CRF model in Assignment 2. The BERT model achieves an f1 score of 0.946 for \"Out of Scope\" and 0.881 for \"In Scope,\" compared to the CRF model's 0.90 and 0.79 respectively. This could indicate that the BERT model is more capable of understanding contextual information through its transformer architecture, which the CRF model lacks. Notably, the 0.79 recall score for \"In Scope\" in the CRF model is lower than the 0.866 recall score in the BERT model, same pattern is noticed in the \"In Scope\" precision score (0.79 and 0.897), showing that BERT identifies more true positives in this category while also reducing false positives.\n",
    "\n",
    "When compared to my teammates' BERT models, my model slightly lags in certain metrics. For example, my model's macro average f1 score of 0.913 is slightly lower than the models with seeds 71, 127, and 4294967294, which achieve macro average f1 scores of 0.916, 0.916, and 0.917 respectively. My model also has the lowest precision for the \"In Scope\" class (0.897), compared to the models with seeds 71 (0.920), 127 (0.910), and 4294967294 (0.915). On the other hand, the highest \"In Scope\" recall comes from my model (0.866). With reference to the confusion matrices, number of false positive cases in my model is the highest among all the others. The reason behind these differences could be that my model is with a random seed that makes it more effective at correctly identifying cases where a token truly belongs to the \"In Scope\" class, at the expense of slightly higher false positives. However, my model shows balanced performance, with relatively stable precision and recall, and a strong weighted f1 score of 0.925.\n",
    "\n",
    "On the other hand, the results of my BERT model could also be compared to the scope token evaluation of the shared task, since my token-level evaluation measure align closely with the Scope Tokens F1 measure. The systems with best performance in the closed track of the shared task reported f1 scores of approximately 0.83 to 0.85, depending on the differences of system (e.g. UiO1 system reported 0.8526 and UiO2 system reported 0.8373 for scope tokens). My model outperforms these systems in this token-level evaluation context. When looking at the open track Scope Tokens f1 scores, the best-performing system, UiO2, achieved a Scope Tokens f1 score of 0.8220. This indicates that my BERT model significantly outperforms the open track system with the highest score by nearly 6 percentage points. Those differences may due to BERT's nature of being pre-trained on large corpora that enable it to be more sensitive to contextual relationships, and also due to the fact that the shared task don't have gold cue labels and have to detect them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21f8af2-1ce8-4c0c-b1fa-a24ec9175393",
   "metadata": {},
   "source": [
    "## Scope Detection Task on Standalone Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a2540b99-c0db-491c-a409-07250776d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "# model.save_pretrained(\"BERT_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1d1ebc1c-c782-442b-adbf-b1c8e6680d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Tokenizer\\\\tokenizer_config.json',\n",
       " 'Tokenizer\\\\special_tokens_map.json',\n",
       " 'Tokenizer\\\\vocab.txt',\n",
       " 'Tokenizer\\\\added_tokens.json',\n",
       " 'Tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the tokenizer\n",
    "# tokenizer.save_pretrained(\"Tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b897a586-9199-416e-9751-beb23330f26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and the tokenizer\n",
    "# assuming that the model and tokenizer folders are in the same directory as this notebook\n",
    "pokemon_model = AutoModelForTokenClassification.from_pretrained(\"BERT_model\")\n",
    "pokemon_tokenizer = AutoTokenizer.from_pretrained(\"Tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fd99e096-2934-4952-86d4-e9717c50a3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trainer\n",
    "# with open(\"BERT_trainer.pkl\", \"wb\") as trainer_file:\n",
    "#     pickle.dump(trainer, trainer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "de1aaa7a-77f7-4030-8d44-e70eca93f3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this function before loading the trainer\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for token classification predictions.\n",
    "\n",
    "    This function calculates precision, recall, f1 score, and accuracy for token-level\n",
    "    predictions. It handles special tokens by removing ignored indices (-100) before computation.\n",
    "\n",
    "    Argument:\n",
    "        p (tuple): A tuple containing:\n",
    "            - predictions (numpy.ndarray): model predictions with shape (batch_size, sequence_length, num_labels)\n",
    "            - labels (numpy.ndarray): ground truth labels with shape (batch_size, sequence_length)\n",
    "                Special tokens are marked with -100 and will be ignored in evaluation\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the following metrics:\n",
    "            - precision (float): overall precision across all classes\n",
    "            - recall (float): overall recall across all classes\n",
    "            - f1 (float): overall f1 score across all classes\n",
    "            - accuracy (float): overall accuracy across all classes\n",
    "\n",
    "    Notes:\n",
    "        - The function expects label_list to be defined in the outer scope\n",
    "        - The metric object should be defined in the outer scope and support the\n",
    "          compute() method with predictions and references parameters\n",
    "        - Predictions are converted from logits to label indices using argmax\n",
    "        - Special tokens (labeled as -100) are removed before metric computation\n",
    "    \"\"\"\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6fb3261c-e049-477e-88de-78a9e45351c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trainer\n",
    "with open(\"BERT_trainer.pkl\", \"rb\") as trainer_file:\n",
    "    pokemon_trainer = pickle.load(trainer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c4c1421a-22f6-4d14-b0f7-d2e5bc0a6bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mega_function(sentence_data, pokemon_tokenizer):\n",
    "    \"\"\"\n",
    "    This is a function combining key components of functions including convert_df, augment_data, tokenized_input, preprocess_example, make_dataset.\n",
    "    Specially designed for negation scope detection using trained BERT model on standalone sentences.\n",
    "\n",
    "    Parameters:\n",
    "    sentence_data : pandas.DataFrame\n",
    "        containing sentence-level data with the following columns:\n",
    "        - 'word': tokens in the sentence.\n",
    "        - 'cue': binary integer indicators for negation cues (1 means the token is a cue, 0 means the token is not a cue).\n",
    "    pokemon_tokenizer: tokenizer added with special negation tokens\n",
    "\n",
    "    Returns:\n",
    "    tuple:\n",
    "        - tokenized_data: list of dict\n",
    "            A list of dictionaries where each dictionary contains the following keys:\n",
    "            - 'input_ids'\n",
    "            - 'attention_mask'\n",
    "            - 'word_ids'\n",
    "            - 'tokenized_tokens'\n",
    "        - final_dataset: datasets.Dataset\n",
    "            A Hugging Face Dataset object containing the tokenized input data for model training, containing:\n",
    "            - 'input_ids'\n",
    "            - 'attention_mask'\n",
    "    \"\"\"\n",
    "    x = [\n",
    "            {\n",
    "                # 'id': '1',\n",
    "                'tokens': sentence_data['word'].tolist(),\n",
    "                'cues': [0 if val == '_' else 1 for val in sentence_data['cue']],\n",
    "                'og_cues': sentence_data['cue'].tolist()\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    y = []\n",
    "    for sentence in x:\n",
    "        \n",
    "        # extract relevant lists\n",
    "        tokens = sentence['tokens']\n",
    "        cues = sentence['cues']\n",
    "        og_cues = sentence['og_cues']\n",
    "    \n",
    "        # count the number of 1s in the cues -- 1 indicates the cue\n",
    "        cue_count = cues.count(1)\n",
    "    \n",
    "        # Process tokens based on the cues\n",
    "        processed_tokens = []\n",
    "        processed_cues = []\n",
    "        processed_og_cues = []\n",
    "        for idx, (token, cue) in enumerate(zip(tokens, cues)):\n",
    "            if cue == 1:\n",
    "                processed_cues.append(cues[idx])\n",
    "                processed_og_cues.append(og_cues[idx])\n",
    "                # multiple cues: mark as [MULTI]\n",
    "                if cue_count > 1:\n",
    "                    processed_tokens.append(\"[MULTI]\")\n",
    "                # single cue: mark based on position\n",
    "                elif og_cues[idx] == token:\n",
    "                    processed_tokens.append(\"[NEG]\")  \n",
    "                else:\n",
    "                    if token.startswith(og_cues[idx]):\n",
    "                        processed_tokens.append(\"[PRE]\")\n",
    "                    else:\n",
    "                        processed_tokens.append(\"[POST]\")\n",
    "            processed_tokens.append(token)\n",
    "            processed_cues.append(cues[idx])\n",
    "            processed_og_cues.append(og_cues[idx])\n",
    "    \n",
    "        y.append({\n",
    "            'tokens':processed_tokens,\n",
    "            'cues': processed_cues,\n",
    "            'og_cues': processed_og_cues\n",
    "        })\n",
    "\n",
    "    tokenized_data=[]\n",
    "    for sent in y:\n",
    "        single_sent={}\n",
    "        tokenized_input = pokemon_tokenizer(sent[\"tokens\"], is_split_into_words=True)\n",
    "        tokens = pokemon_tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "        word_ids = tokenized_input.word_ids()\n",
    "        \n",
    "        single_sent[\"input_ids\"] = tokenized_input[\"input_ids\"]\n",
    "        single_sent[\"attention_mask\"] = tokenized_input[\"attention_mask\"]\n",
    "        single_sent['word_ids'] = word_ids\n",
    "        single_sent['tokenized_tokens'] = tokens\n",
    "        tokenized_data.append(single_sent)\n",
    "\n",
    "    \n",
    "    processed_data = []\n",
    "    for example in tokenized_data:\n",
    "        processed_data.append({\n",
    "            \"input_ids\": tokenized_data[0][\"input_ids\"],\n",
    "            \"attention_mask\": tokenized_data[0][\"attention_mask\"],\n",
    "        })\n",
    "\n",
    "        \n",
    "    final_dataset = Dataset.from_dict({\n",
    "        'input_ids': [example['input_ids'] for example in processed_data],\n",
    "        'attention_mask': [example['attention_mask'] for example in processed_data]\n",
    "    })\n",
    "    \n",
    "    return tokenized_data, final_dataset\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8937f7ff-cfb3-4d5c-9a74-034e7868b6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_ids': [101, 2016, 2038, 30525, 1050, 1005, 1056, 2736, 2014, 19453, 2664, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'word_ids': [None, 0, 1, 2, 3, 3, 3, 4, 5, 6, 7, 8, None], 'tokenized_tokens': ['[CLS]', 'she', 'has', '[NEG]', 'n', \"'\", 't', 'finished', 'her', 'homework', 'yet', '.', '[SEP]']}]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " She n't homework .\n"
     ]
    }
   ],
   "source": [
    "def transform_to_test_sentence(sent, target, pokemon_trainer, pokemon_model, pokemon_tokenizer):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters:\n",
    "    sent (list of strings): tokens of a sentence. \n",
    "    target (list of strings): list containing '0' and '1' as marking negation cues. '1' means the token is a negation cue.\n",
    "    pokemon_trainer\n",
    "    pokemon_model\n",
    "    pokemon_tokenizer: custom tokenizer with added special negation tokens\n",
    "\n",
    "    Returns:\n",
    "    A list of dictionaries of predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(sent) != len(target):\n",
    "        raise ValueError(\n",
    "            \"The length of the sentence and the array to mark cue must be the same.\")\n",
    "\n",
    "    # creates list of dicts for word and cue\n",
    "    test_sentence = []\n",
    "    for token, label in zip(sent, target):\n",
    "        test_sentence.append(\n",
    "            {\"word\": token, \"cue\": token if label == \"1\" else '_'})\n",
    "\n",
    "    # converts list of dicts to dataframe\n",
    "    sentence_data = pd.DataFrame(test_sentence)\n",
    "    tokenized_data, final = mega_function(sentence_data, pokemon_tokenizer)\n",
    "\n",
    "    print(tokenized_data)\n",
    "    predictions, _, _ = pokemon_trainer.predict(final)\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    for idx, i in enumerate(tokenized_data):\n",
    "        # slice the predictions to match the actual length of the sentence (ignoring padding tokens)\n",
    "        length = len(i['word_ids'])\n",
    "        i['predictions'] = predictions[idx][:length] # assign the model's predicted labels for this sentence\n",
    "\n",
    "    processed_predictions = []\n",
    "    for data in tokenized_data:\n",
    "        word_ids = data['word_ids']\n",
    "        predictions = data['predictions']\n",
    "        \n",
    "        # filter out elements where labels are -100\n",
    "        word_ids = [b for a, b in zip(data['tokenized_tokens'], word_ids) if a not in ['[CLS]','[SEP]','[PAD]','[NEG]','[MULTI]','[PRE]','[POST]','[UNK]','[MASK]']]\n",
    "        predictions = [b for a, b in zip(data['tokenized_tokens'], predictions) if a not in ['[CLS]','[SEP]','[PAD]','[NEG]','[MULTI]','[PRE]','[POST]','[UNK]','[MASK]']]\n",
    "\n",
    "        # initialize variables to track the current group\n",
    "        current_word_id = None\n",
    "        current_predictions = []\n",
    "        \n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id != current_word_id:\n",
    "                # if a new word_id starts, calculate majority vote for the previous group\n",
    "                if current_predictions:\n",
    "                    processed_predictions.append(Counter(current_predictions).most_common(1)[0][0])\n",
    "                \n",
    "                # reset for the new word_id\n",
    "                current_word_id = word_id\n",
    "                current_predictions = [predictions[idx]]\n",
    "            else:\n",
    "                # add to the current group\n",
    "                current_predictions.append(predictions[idx])\n",
    "        \n",
    "        # handle the last group\n",
    "        if current_predictions:\n",
    "            processed_predictions.append(Counter(current_predictions).most_common(1)[0][0])\n",
    "\n",
    "    return processed_predictions\n",
    "    \n",
    "\n",
    "sent = [\"She\", \"has\", \"n't\", \"finished\", \"her\", \"homework\", \"yet\", \".\"]\n",
    "sent_target = [\"0\", \"0\", \"1\", \"0\", \"0\", \"0\", \"0\", \"0\"]\n",
    "\n",
    "scope = transform_to_test_sentence(sent, sent_target, pokemon_trainer, pokemon_model, pokemon_tokenizer)\n",
    "# pprint(scope)\n",
    "str_scope = ''\n",
    "for idx , i in enumerate(sent):\n",
    "    if scope[idx] == 1:\n",
    "        str_scope += \" \"+i\n",
    "\n",
    "print(str_scope)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
